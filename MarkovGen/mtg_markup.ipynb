{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Markov Text Generator\n",
    "### Project Requirements\n",
    "* Implement a finish_sentence(sentence,n,corpus,randomize = False) function\n",
    "    Args:\n",
    "        * sentence: List of tokens to build on\n",
    "        * n: Integer indicating the length of n-gram modeled \n",
    "        * corpus: list of tokens for the entire corpus. Test module will run on Jane Austen's 'Sense and Sensibility'\n",
    "        * randomize: flag indicated deterministic word selection (false) or stochastic (true)\n",
    "\n",
    "This is a simple Markov Text Generator which will implement stupid backoff and generate a sentence of words from a sentence prior until:\n",
    "* Punctuation is encountered ('?','!','.')\n",
    "* The final sentence length reaches 10 tokens total (indluding the given prior)\n",
    "\n",
    "In the project assignment, the requirement for a weighted backoff coefficient of one was indicated. It was found during implementation that this resulted in the inappropriate weighting of lower order n-grams to outperform higher order n-grams. This was problematic because this backoff coefficient does not penalize the loss of context/information that results from backing off to a lower order n-gram. Therefore, this model opts for an alpha value of 0.4 to appropriately penalize lower order n-gram probabilities.\n",
    "\n",
    "### Below is a segmented portion of the model with descriptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project utilizes defaultdict from the default python library and numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first key function parses the corpus into all relevant n-grams from the unigram to the n-gram and its associated counts as a key value pair in a dictionary of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_counter(corpus, n):\n",
    "    \"\"\"generates a dictionary of dictionaries for all n grams in the corpus with an n*len(corpus) efficiency\"\"\"\n",
    "    ngram_counter = defaultdict(dict)\n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(n + 1):\n",
    "            ngram = tuple(corpus[i : i + j])\n",
    "            ngram_counter[j][ngram] = ngram_counter[j].get(ngram, 0) + 1\n",
    "    return dict(ngram_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next key function is a recursively called pobability computing function. This function will compute the probability based of count ratios from the counter dictionary matrix and, if an n-gram count is zero, call itself again on a backoff n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob(ngram, prob_mat, corpus):\n",
    "    \"\"\"uses the counter matrix to determine the next best word\"\"\"\n",
    "    probability = 0\n",
    "    # recursive base case\n",
    "    n = len(ngram)\n",
    "    prior = ngram[:-1]\n",
    "    if n == 1:\n",
    "        probability = prob_mat[n][ngram] / len(corpus)\n",
    "        return probability\n",
    "\n",
    "    if ngram in prob_mat[n].keys():\n",
    "        num = prob_mat[n][ngram]\n",
    "        denom = prob_mat[n - 1][prior]\n",
    "        probability = num / denom\n",
    "    else:\n",
    "        probability = 0.4 * compute_prob(ngram[1:], prob_mat, corpus)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The operative function for this project is the finish sentence function. This function iterates through each vocabulary word, assigns a probability to the n-gram created by assigning that word to the prior from the input sentence, and selects the highest probability word (deterministically or stochastically based of the randomize flag) until our sentence completion criterion are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_sentence(sentence, n, corpus, randomize=False):\n",
    "    \"\"\"this function will compute probabilities based off prior sentences and pick the best word\"\"\"\n",
    "    v, ind = np.unique(np.array(corpus), return_index=True)\n",
    "    vocab = v[np.argsort(ind)]\n",
    "    final_sentence = np.array(sentence)\n",
    "    vocabulary = vocab.tolist()\n",
    "\n",
    "    count_matrix = n_gram_counter(corpus, n)\n",
    "    # print(count_matrix[1])\n",
    "    # print(f\"Count of not be: {count_matrix[2][('not','be')]}\")\n",
    "    # print(f\"Count of not: {count_matrix[1][('not',)]}\")\n",
    "    # print(f\"Count of was not in: {count_matrix[3][('was', 'not', 'in')]}\")\n",
    "    # print(f\"Count of was not: {count_matrix[2][('was', 'not')]}\")\n",
    "\n",
    "    best_word_indx = 0\n",
    "    while (\n",
    "        # run until we get to a sentence of 10 tokens or punctuation\n",
    "        len(final_sentence) < 10\n",
    "        and vocabulary[best_word_indx] != \".\"\n",
    "        and vocabulary[best_word_indx] != \"!\"\n",
    "        and vocabulary[best_word_indx] != \"?\"\n",
    "    ):\n",
    "        if n > len(final_sentence):\n",
    "            prior = final_sentence\n",
    "            pass\n",
    "        else:\n",
    "            prior = final_sentence[-(n - 1) :]\n",
    "            pass\n",
    "        best_word_indx = 0\n",
    "        prob = 0\n",
    "        equal_words = []\n",
    "        # pylint: disable = Consider using enumerate instead of iterating with range and lenPylintC0200:consider-using-enumerate\n",
    "        for i in range(len(vocabulary)):\n",
    "            # append vocabulary words onto prior and compute the backoff probability\n",
    "            curr_gram = tuple(np.append(prior, vocabulary[i]))\n",
    "            if n == 1:\n",
    "                curr_gram = (vocabulary[i],)\n",
    "            # print(f\"computing probability for {curr_gram} \")\n",
    "            curr_prob = compute_prob(curr_gram, count_matrix, corpus)\n",
    "\n",
    "            if curr_prob > prob:  # handles the deterministic case\n",
    "                prob = curr_prob\n",
    "                best_word_indx = i\n",
    "            elif curr_prob == prob and randomize is True:\n",
    "                # create a list of words of equal probability to select from\n",
    "                equal_words.append(vocabulary[i])\n",
    "                pass\n",
    "            else:\n",
    "                pass\n",
    "        if len(equal_words) == 0:  # no words of equal probability found\n",
    "            final_sentence = np.append(final_sentence, vocabulary[best_word_indx])\n",
    "        else:  # take a random selection from the words of equal probability\n",
    "            eq_words = np.array(equal_words)\n",
    "            word_chosen = np.random.choice(eq_words)\n",
    "            final_sentence = np.append(final_sentence, word_chosen)\n",
    "    return final_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
